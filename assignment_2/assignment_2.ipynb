{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, BasicProblem, generate_categories\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data\n",
    "We are going to work with the [Folktables](https://github.com/socialfoundations/folktables#quick-start-examples) dataset (*you have already worked with it*).\n",
    "\n",
    "1. As last week, we are still predicting the *Total person's income*  (I've digitized  it in  `target_transform=lambda x: x > 25000`).\n",
    "2. Today, we are going to implement two methods for data debiasing: [Fair PCA](https://deepai.org/publication/efficient-fair-pca-for-fair-representation-learning) and [A Geometric Solution to Fair Representations](https://dl.acm.org/doi/10.1145/3375627.3375864).\n",
    "3. We are going to evaluate the performance on two sensitive features: `SEX` (i.e. *Males* and *Females*) and `RAC1P` (we will consider only *Whites* and *African-Americans*)\n",
    "4. I updated the filtering method `adult_filter` to keep the specified groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "def adult_filter(data):\n",
    "    \"\"\"Mimic the filters in place for Adult data.\n",
    "    Adult documentation notes: Extraction was done by Barry Becker from\n",
    "    the 1994 Census database. A set of reasonably clean records was extracted\n",
    "    using the following conditions:\n",
    "    ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    \"\"\"\n",
    "    df = data\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    df = df[df[\"RAC1P\"] < 3] ## keep only Whites and African-Americans\n",
    "    return df\n",
    "\n",
    "\n",
    "ACSIncomeNew = BasicProblem(\n",
    "    features=[\n",
    "        'AGEP',\n",
    "        'COW',\n",
    "        'SCHL',\n",
    "        'MAR',\n",
    "        'CIT',\n",
    "        'RELP',\n",
    "        'WKHP',\n",
    "        'SEX',\n",
    "        'RAC1P'\n",
    "    ],\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 25000,    \n",
    "    group=['SEX', \"RAC1P\"],\n",
    "    preprocess=adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n",
    "# acs_data = acs_data.sample(500, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = ACSIncomeNew.df_to_pandas(acs_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data[0], data[1], data[2], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "african_american = X_train[X_train['RAC1P'] == 2]\n",
    "whites = X_train[X_train['RAC1P'] == 1]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "african_american = resample(african_american,\n",
    "                            replace=True,\n",
    "                            n_samples=len(whites),\n",
    "                            random_state=0)\n",
    "oversampled_df = pd.concat([african_american,whites]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot feature distributions for both genders in one plot\n",
    "fig, axes = plt.subplots(7, figsize=(20, 40))\n",
    "for i, feature in enumerate(['AGEP', 'COW', 'SCHL', 'SEX', 'CIT', 'MAR','WKHP']):\n",
    "    sns.histplot(data=oversampled_df, x=feature, hue='RAC1P', ax=axes[i], palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dython\n",
    "dython.nominal.associations(X_train, nominal_columns=['COW', 'CIT', 'RAC1P', 'SEX', 'RELP', 'MAR'], mark_columns=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(acs_data, categories=categories, dummies=True)\n",
    "### groups now contain information about SEX and RAC1P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"redundant\" columns\n",
    "features = features.drop([\"RELP_Unmarried partner\",\n",
    "                          \"CIT_U.S. citizen by naturalization\",\n",
    "                          \"SEX_Male\",\n",
    "                          \"SCHL_1 or more years of college credit, no degree\",  \n",
    "                          \"MAR_Divorced\", \n",
    "                          \"RELP_Adopted son or daughter\",\n",
    "                          'COW_Working without pay in family business or farm', \n",
    "                          \"RAC1P_White alone\" ], axis = 1) \n",
    "\n",
    "print(\"Columns with the protected features:\")\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"RAC1P\" in f) or (\"SEX\" in f):\n",
    "        print(\"Column ID: %s\" %i, \"(%s)\"%f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "N = 500 ### I am subsampling because it is slow on my machine\n",
    "\n",
    "X_train = X_train[:N]\n",
    "y_train = y_train[:N]\n",
    "group_train = group_train[:N]\n",
    "X_test = X_test[:N]\n",
    "y_test = y_test[:N]\n",
    "group_test = group_test[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## last columns of our data contains the protected features\n",
    "protected = X_train[:,56:] \n",
    "nonprotected = X_train[:,:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2.\n",
    "Use the following arguments in the `opt.fmin_funct`: `xtol=1e-4, ftol=1e-4,  maxfun=1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.logspace(1e-5,1e-2,10)\n",
    "###########\n",
    "# YOUR CODE\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    This is logistic regression\n",
    "    f = 1/(1+exp(-beta^T * x))\n",
    "    This function assumes as input that you have already multiplied beta and X together\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps = 1e-9):\n",
    "    \"\"\"\n",
    "    Loss for the logistic regression, y_preds are probabilities\n",
    "    eps: epsilon for stability\n",
    "    \"\"\"\n",
    "    # check whether that's the same as 1/m * sum()\n",
    "    return -np.mean(y_true*np.log(y_pred + eps)+(1-y_true)*np.log(1-y_pred + eps))\n",
    "    \n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2-Regularisation\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(beta)\n",
    "\n",
    "def compute_gradient(beta, X, y_true, _gamma): # y = y_true\n",
    "    \"\"\"Calculate the gradient - used for finding the best beta values. \n",
    "       You do not need to use groups and lambda (fmin_tnc expects same input as in func, that's why they are included here)\"\"\"\n",
    "    grad = np.zeros(beta.shape)\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.array(sigmoid(X.dot(beta))) # same as beta @ X.T?\n",
    "    \n",
    "    for i in range(len(grad)):\n",
    "        if i == 0:\n",
    "            # don't want to penalize the intercept\n",
    "            grad[i] = (1/m) * (y_pred - y_true).dot(X[:,i]) \n",
    "        else:\n",
    "            # start with beta[1]\n",
    "            grad[i] = (1/m) * (y_pred - y_true).dot(X[:,i]) + 2* _gamma *beta[i] \n",
    "    #print(f'Gradient: {grad}')\n",
    "    return grad\n",
    "\n",
    "def compute_cost(beta , X, y_true, grouping, _lambda, _gamma):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "    y_pred = sigmoid(X.dot(beta)) \n",
    "    cost = logistic_loss(y_true, y_pred) + _gamma*l2_loss(beta[1:]) # to not penalize the Intercept?\n",
    "    #print(f'Cost: {cost}')\n",
    "    return cost\n",
    "\n",
    "beta = np.random.rand(X_train.shape[1])\n",
    "# set parameters\n",
    "lambda_ = 0.1\n",
    "gamma_ = 0.1\n",
    "#\n",
    "X = X_train\n",
    "y = y_train\n",
    "groups = group_train\n",
    "\n",
    "beta = np.random.rand(58)\n",
    "lambdas = np.linspace(1,1e5, 20)\n",
    "\n",
    "########## This is the optimisation function\n",
    "\n",
    "result = opt.fmin_tnc(func=compute_cost, x0=beta, approx_grad = True, maxfun = 1000,\n",
    "                         args = (nonprotected,y_train, groups, lambda_, gamma_), ftol=1e-4, xtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define a list of values for lambda to try\n",
    "lambdas = np.linspace(1, 1e5, 20)\n",
    "\n",
    "# Initialize an array to store the cross-validation errors for each lambda value\n",
    "cv_errors = np.zeros(len(lambdas))\n",
    "\n",
    "# Define the number of folds for K-fold cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Create the KFold cross-validation object\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop over each lambda value\n",
    "for i, lambda_val in enumerate(lambdas):\n",
    "    \n",
    "    # Initialize an array to store the prediction errors for each fold\n",
    "    fold_errors = np.zeros(num_folds)\n",
    "    \n",
    "    # Loop over each fold\n",
    "    for j, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        \n",
    "        # Split the data into training and validation sets for this fold\n",
    "        X_fold_train, y_fold_train = X_train[train_idx], y_train[train_idx]\n",
    "        X_fold_val, y_fold_val = X_train[val_idx], y_train[val_idx]\n",
    "        \n",
    "        # Train the logistic regression model with L2-regularization on the training data for this fold\n",
    "        beta = np.random.rand(X_train.shape[1])\n",
    "        result = opt.fmin_tnc(func=compute_cost, x0=beta, approx_grad=True, maxfun=1000,\n",
    "                              args=(X_fold_train, y_fold_train, group_train, lambda_val, gamma_), \n",
    "                              ftol=1e-4, xtol=1e-4)\n",
    "        beta_opt = result[0]\n",
    "        \n",
    "        # Make predictions on the validation data and calculate the prediction error\n",
    "        y_pred = sigmoid(X_fold_val.dot(beta_opt))\n",
    "        fold_errors[j] = logistic_loss(y_fold_val, y_pred)\n",
    "        \n",
    "    # Calculate the average prediction error for this lambda value\n",
    "    cv_errors[i] = np.mean(fold_errors)\n",
    "    \n",
    "# Find the lambda value with the lowest cross-validation error\n",
    "best_lambda = lambdas[np.argmin(cv_errors)]\n",
    "\n",
    "# Train the logistic regression model on the entire training data with the best lambda value\n",
    "beta = np.random.rand(X_train.shape[1])\n",
    "result = opt.fmin_tnc(func=compute_cost, x0=beta, approx_grad=True, maxfun=1000,\n",
    "                      args=(X_train, y_train, group_train, best_lambda, gamma_), \n",
    "                      ftol=1e-4, xtol=1e-4)\n",
    "beta_opt = result[0]\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = sigmoid(X_test.dot(beta_opt))\n",
    "test_error = logistic_loss(y_test, y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.3\n",
    "Use the following arguments in the `opt.fmin_funct`: ` xtol=1e-3, ftol=1e-3, approx_grad=True, maxfun=1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.array([1e-3, 5e-3, 1e-2, 5e-2, 0.1, 1])\n",
    "###########\n",
    "# YOUR CODE\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE FROM HANNAH\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    This is logistic regression\n",
    "    f = 1/(1+exp(-beta^T * x))\n",
    "    This function assumes as input that you have already multiplied beta and X together\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps = 1e-9):\n",
    "    \"\"\"\n",
    "    Loss for the logistic regression, y_preds are probabilities\n",
    "    eps: epsilon for stability\n",
    "    \"\"\"\n",
    "    # check whether that's the same as 1/m * sum()\n",
    "    return -np.mean(y_true*np.log(y_pred + eps)+(1-y_true)*np.log(1-y_pred + eps))\n",
    "    \n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2-Regularisation\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(beta)\n",
    "\n",
    "def fair_loss(y_true, y_pred, grouping):\n",
    "    \"\"\"\n",
    "    Group fairness Loss\n",
    "    \"\"\"\n",
    "    n = y_true.shape[0]\n",
    "    \n",
    "    # SEX\n",
    "    n1 = np.sum(grouping['SEX'] == 1)\n",
    "    n2 = n - n1\n",
    "\n",
    "    cost_sex = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if y_true[i] == y_true[j]:\n",
    "                if grouping['SEX'].iloc[i] != grouping['SEX'].iloc[j]:\n",
    "                    cost_sex += (y_pred[i] - y_pred[j])**2                \n",
    "    cost_sex = cost_sex/(n1*n2)\n",
    "    # print(cost_sex)\n",
    "    \n",
    "    # RAC1P\n",
    "    n1 = np.sum(grouping['RAC1P'] == 1)\n",
    "    n2 = n - n1\n",
    "\n",
    "    cost_race = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if y_true[i] == y_true[j]:\n",
    "                if grouping['RAC1P'].iloc[i] != grouping['RAC1P'].iloc[j]:\n",
    "                    cost_race += (y_pred[i] - y_pred[j])**2                \n",
    "    cost_race = cost_race/(n1*n2)\n",
    "    # print(cost_race)\n",
    "    \n",
    "    # Is this right? Or how should the two fairness constraints be connected before applying the scaling?\n",
    "    return cost_sex + cost_race\n",
    "\n",
    "\n",
    "### not needed (?)\n",
    "def compute_gradient(beta,X,y_true,_gamma): # y = y_true\n",
    "    \"\"\"Calculate the gradient - used for finding the best beta values. \n",
    "       You do not need to use groups and lambda (fmin_tnc expects same input as in func, that's why they are included here)\"\"\"\n",
    "    grad = np.zeros(beta.shape)\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.array(sigmoid(X.dot(beta))) # same as beta @ X.T?\n",
    "    \n",
    "    for i in range(len(grad)):\n",
    "        if i == 0:\n",
    "            # don't want to penalize the intercept\n",
    "            grad[i] = (1/m) * (y_pred - y_true).dot(X[:,i]) \n",
    "        else:\n",
    "            # start with beta[1]\n",
    "            grad[i] = (1/m) * (y_pred - y_true).dot(X[:,i]) + 2* _gamma *beta[i] \n",
    "    #print(f'Gradient: {grad}')\n",
    "    return grad\n",
    "\n",
    "\n",
    "def compute_cost(beta , X, y_true, grouping, _lambda, _gamma):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "    y_pred = sigmoid(X.dot(beta)) \n",
    "    cost = logistic_loss(y_true, y_pred) + _lambda*fair_loss(y_true, y_pred, grouping) + _gamma*l2_loss(beta[1:]) # to not penalize the Intercept?\n",
    "    #print(f'Cost: {cost}')\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''np.random.seed(0)\n",
    "lambdas = np.array([1e-3, 5e-3, 1e-2, 5e-2, 0.1, 1])\n",
    "betas = np.random.rand(X_train.shape[1])\n",
    "\n",
    "opt.fmin_tnc(func=compute_cost, x0=beta, approx_grad = True, maxfun = 1000,\n",
    "                         args = (nonprotected,y_train, groups, lambdas[0], gammas[2]), ftol=1e-3, xtol=1e-3)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Fair PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "#categories = generate_categories(features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(acs_data, categories=categories, dummies=True)\n",
    "\n",
    "# Drop the \"redundant\" columns\n",
    "features = features.drop([\"RAC1P_White alone\", \n",
    "                          \"SEX_Male\", \n",
    "                          \"SCHL_1 or more years of college credit, no degree\",  \n",
    "                          \"MAR_Divorced\", \n",
    "                          \"RELP_Adopted son or daughter\",\n",
    "                          'COW_Working without pay in family business or farm' ], axis = 1) \n",
    "\n",
    "print(\"Columns with the protected features:\")\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"RAC1P\" in f) or (\"SEX\" in f):\n",
    "        print(\"Column ID: %s\" %i, \"(%s)\"%f)\n",
    "\n",
    "### groups now contain information about SEX and RAC1P\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "N = 500 ### I am subsampling because it is slow on my machine\n",
    "\n",
    "X_train = X_train[:N]\n",
    "y_train = y_train[:N]\n",
    "group_train = group_train[:N]\n",
    "X_test = X_test[:N]\n",
    "y_test = y_test[:N]\n",
    "group_test = group_test[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = X_train[:,-2:]\n",
    "nonprotected = X_train[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# normalize first two columns for age and wkhp\n",
    "non_protected_features_scaled = nonprotected.copy()\n",
    "non_protected_features_scaled[:,:2] = scaler.fit_transform(nonprotected[:,:2])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=len(features.columns)-4) # - protected features\n",
    "X_pca = pca.fit_transform(non_protected_features_scaled)\n",
    "\n",
    "# pca.explained_variance_ratio_\n",
    "# scree plot bar plot\n",
    "plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_*100)\n",
    "for i in range(len(pca.explained_variance_ratio_)):\n",
    "    plt.text(i+0.7, pca.explained_variance_ratio_[i]*100+0.3, str(round(pca.explained_variance_ratio_[i]*100,2))+'%')\n",
    "plt.xlabel('Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first three components interactive 3d\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=X_pca[:,0],\n",
    "    y=X_pca[:,1],\n",
    "    z=X_pca[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=['red' if x else 'blue' for x in y_train], # set color to an array/list of desired values\n",
    "        colorscale='Viridis',   # choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr matrix comparing protected\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.concatenate((protected, X_pca), axis=1))\n",
    "\n",
    "corr = round(df.corr(),2)\n",
    "corr.columns=['SEX', 'RAC1P', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24', 'PC25', 'PC26', 'PC27', 'PC28', 'PC29', 'PC30', 'PC31', 'PC32', 'PC33', 'PC34', 'PC35', 'PC36', 'PC37', 'PC38', 'PC39', 'PC40', 'PC41', 'PC42', 'PC43', 'PC44', 'PC45', 'PC46', 'PC47', 'PC48', 'PC49', 'PC50', 'PC51', 'PC52', 'PC53', 'PC54', 'PC55', 'PC56']\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "            cmap='RdBu_r',\n",
    "            # annot=True,\n",
    "            linewidth=0.5);\n",
    "\n",
    "print(f'WE SEE THAT THERE ARE NO CORRELATION BETWEEN ANY OF THE PCA COMPONENTS, WHICH MAKES SENSE SINCE PRINCIPAL COMPONENTS TRY TO COVER DIFFERENT ORTHAGONAL DIRECTIONS AND VARIANCE IN THE DATA.')\n",
    "print(f'Also Sex is slightly correlated PC3 and PC4, which can mean that they are somewhat proxies.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project test data with pca\n",
    "nonprotected_test = X_test[:,:-2]\n",
    "non_protected_features_scaled_test = nonprotected_test.copy()\n",
    "\n",
    "# normalize first two columns for age and wkhp\n",
    "non_protected_features_scaled_test[:,:2] = scaler.transform(nonprotected_test[:,:2])\n",
    "X_pca_test = pca.transform(non_protected_features_scaled_test)\n",
    "\n",
    "# project it back into the original space\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "X_reconstructed_test = pca.inverse_transform(X_pca_test)\n",
    "\n",
    "# calculate reconstruction error for each sample as mean absolute error\n",
    "reconstruction_error = []\n",
    "for i in range(len(non_protected_features_scaled)):\n",
    "    reconstruction_error.append(np.mean(np.abs(non_protected_features_scaled[i] - X_reconstructed[i])))\n",
    "reconstruction_error_test = []\n",
    "for i in range(len(non_protected_features_scaled_test)):\n",
    "    reconstruction_error_test.append(np.mean(np.abs(non_protected_features_scaled_test[i] - X_reconstructed_test[i])))\n",
    "\n",
    "# get reconstruction error for protected features\n",
    "female_error = []\n",
    "male_error = []\n",
    "whites_error = []\n",
    "african_american_error = []\n",
    "for i in range(len(reconstruction_error_test)):\n",
    "    # person is always member of both protected groups\n",
    "\n",
    "    # 1 is male and 2 is female\n",
    "    if protected[i][0] == 1:\n",
    "        male_error.append(reconstruction_error_test[i])\n",
    "    else:\n",
    "        female_error.append(reconstruction_error_test[i])\n",
    "\n",
    "    # 1 is white and 2 is african american\n",
    "    if protected[i][1] == 1:\n",
    "        whites_error.append(reconstruction_error_test[i])\n",
    "    else:\n",
    "        african_american_error.append(reconstruction_error_test[i])\n",
    "\n",
    "# calculate mean reconstruction error for each group\n",
    "print(np.mean(male_error))\n",
    "print(np.mean(female_error))\n",
    "print(np.mean(whites_error))\n",
    "print(np.mean(african_american_error))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 Fair PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X = non_protected_features_scaled.copy()\n",
    "# create Matrix Z with protected features\n",
    "Z = protected.copy()\n",
    "# remove mean from each column\n",
    "Z = Z - np.mean(Z, axis=0)\n",
    "# find orthonormal null space spanned by ZTX with scipy.linalg.null_space\n",
    "R = scipy.linalg.null_space(Z.T @ X)\n",
    "\n",
    "# Find orthonormal eigenvectors RTXTXR with scipy.linalg.eig\n",
    "eigvals, eigvecs = scipy.linalg.eig(R.T @ X.T @ X @ R)\n",
    "# sort eigenvectors by eigenvalues\n",
    "idx = eigvals.argsort()[::-1]\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:,idx]\n",
    "\n",
    "# get matrix of first 50 eigenvectors\n",
    "L = eigvecs[:,:5]\n",
    "\n",
    "# projection matrix U = RL\n",
    "U = R @ L\n",
    "\n",
    "# project data\n",
    "X_projected = X @ U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# see if X_projected is correlated with Z as heatmap\n",
    "sns.heatmap(np.corrcoef(X_projected.T, Z.T), cmap='RdBu_r', linewidth=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project test data\n",
    "X_test_projected = non_protected_features_scaled_test @ U\n",
    "\n",
    "# reproject it back into the original space\n",
    "X_reconstructed = X_projected @ U.T\n",
    "X_reconstructed_test = X_test_projected @ U.T\n",
    "\n",
    "# calculate reconstruction error for each sample as mean absolute error\n",
    "reconstruction_error = []\n",
    "for i in range(len(non_protected_features_scaled)):\n",
    "    reconstruction_error.append(np.mean(np.abs(non_protected_features_scaled[i] - X_reconstructed[i])))\n",
    "reconstruction_error_test = []\n",
    "for i in range(len(non_protected_features_scaled_test)):\n",
    "    reconstruction_error_test.append(np.mean(np.abs(non_protected_features_scaled_test[i] - X_reconstructed_test[i])))\n",
    "# get reconstruction error for protected features\n",
    "female_error = []\n",
    "male_error = []\n",
    "whites_error = []\n",
    "african_american_error = []\n",
    "for i in range(len(reconstruction_error_test)):\n",
    "    # person is always member of both protected groups\n",
    "\n",
    "    # 1 is male and 2 is female\n",
    "    if protected[i][0] == 1:\n",
    "        male_error.append(reconstruction_error_test[i])\n",
    "    else:\n",
    "        female_error.append(reconstruction_error_test[i])\n",
    "\n",
    "    # 1 is white and 2 is african american\n",
    "    if protected[i][1] == 1:\n",
    "        whites_error.append(reconstruction_error_test[i])\n",
    "    else:\n",
    "        african_american_error.append(reconstruction_error_test[i])\n",
    "\n",
    "# calculate mean reconstruction error for each group\n",
    "print(np.mean(male_error))\n",
    "print(np.mean(female_error))\n",
    "print(np.mean(whites_error))\n",
    "print(np.mean(african_american_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit own implementation of logistic regression on projected data\n",
    "\n",
    "beta = np.random.rand(5)\n",
    "\n",
    "result = opt.fmin_tnc(func=compute_cost, x0=beta, approx_grad = True, maxfun = 1000,\n",
    "                         args = (X_projected, y_train, groups, lambda_, gamma_), ftol=1e-4, xtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data by multiplying with result vector\n",
    "y_pred = X_test_projected @ result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
