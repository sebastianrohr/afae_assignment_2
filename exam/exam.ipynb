{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Fair PCA on different datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import dython\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recidivism = pd.read_csv(f'data/propublica_data_for_fairml.csv')\n",
    "\n",
    "df_recidivism['Caucasian'] = ((df_recidivism['Other'] == 0) & (df_recidivism['African_American'] == 0) & (\n",
    "    df_recidivism['Asian'] == 0) & (df_recidivism['Hispanic'] == 0) & (df_recidivism['Native_American'] == 0)).astype(int)\n",
    "df_recidivism['Between_TwentyFive_And_FourtyFive'] = (\n",
    "    (df_recidivism['Age_Above_FourtyFive'] == 0) & (df_recidivism['Age_Below_TwentyFive'] == 0)).astype(int)\n",
    "df_recidivism['Male'] = (df_recidivism['Female'] == 0).astype(int)\n",
    "\n",
    "# revert one hot encoding\n",
    "races = ['Other', 'African_American', 'Asian', 'Hispanic', 'Native_American', 'Caucasian']\n",
    "df_recidivism['Race'] = df_recidivism[races].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(races, axis=1)\n",
    "\n",
    "genders = ['Female', 'Male']\n",
    "df_recidivism['Gender'] = df_recidivism[genders].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(genders, axis=1)\n",
    "\n",
    "age_group = ['Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'Between_TwentyFive_And_FourtyFive']\n",
    "df_recidivism['Age_Group'] = df_recidivism[age_group].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(age_group, axis=1)\n",
    "\n",
    "df_recidivism = df_recidivism.drop('score_factor', axis=1)\n",
    "df_recidivism = df_recidivism[df_recidivism[\"Race\"].isin([\"African_American\", \"Caucasian\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"Race\", \"Gender\", \"Age_Group\"]\n",
    "dython.nominal.associations(df_recidivism, nominal_columns=cat_cols, mark_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distributions for both genders in one plot\n",
    "fig, axes = plt.subplots(7, figsize=(20, 40))\n",
    "for i, feature in enumerate(df_recidivism.columns):\n",
    "    sns.histplot(data=df_recidivism, x=feature, hue='Race', ax=axes[i], palette='Set2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode data\n",
    "dummies_df_recividism = pd.get_dummies(df_recidivism, columns=cat_cols, drop_first=True)\n",
    "# remove target variable from features\n",
    "labels = dummies_df_recividism.Two_yr_Recidivism\n",
    "features = dummies_df_recividism.drop(\"Two_yr_Recidivism\", axis=1)\n",
    "\n",
    "features = features[[\"Number_of_Priors\", \"Misdemeanor\", \"Age_Group_Age_Below_TwentyFive\",\n",
    "                     \"Age_Group_Between_TwentyFive_And_FourtyFive\", \"Race_Caucasian\", \"Gender_Male\"]]\n",
    "\n",
    "# identify protected groups\n",
    "indices = []\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"Race\" in f) or (\"Gender\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n",
    "        indices.append(i)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "groups = features.iloc[:, indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last columns of our data contains the protected features\n",
    "protected = X_train[:, -2:]\n",
    "nonprotected = X_train[:, :-2]\n",
    "\n",
    "protected_test = X_test[:, -2:]\n",
    "nonprotected_test = X_test[:, :-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        Recall of the positive class in binary classification or weighted\n",
    "        average of the recall of each class for the multiclass task.\n",
    "    precision : float\n",
    "        Precision of the positive class in binary classification or weighted\n",
    "        average of the precision of each class for the multiclass task.\n",
    "    f1_score : float\n",
    "        F1 score of the positive class in binary classification or weighted\n",
    "        average of the F1 score of each class for the multiclass task.\n",
    "    accuracy : float\n",
    "        Accuracy of the positive class in binary classification or weighted\n",
    "        average of the accuracy of each class for the multiclass task.\n",
    "    \"\"\"\n",
    "\n",
    "    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n",
    "    FP = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n",
    "    TN = np.sum(np.logical_and(y_pred == 0, y_true == 0))\n",
    "    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP/(TP+FN) + TN/(TN+FP)) / 2\n",
    "\n",
    "    metrics_dict = {'recall': recall,\n",
    "                    'precision': precision,\n",
    "                    'f1_score': f1_score,\n",
    "                    'accuracy': accuracy}\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate statistical parity, equalized odds and equalized outcome for all groups\n",
    "def calculate_fairness_metrics(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Calculate statistical parity, equalized odds and equalized outcome for all groups\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for group in groups:\n",
    "        for i in [0, 1]:\n",
    "            group_idx = np.where(groups[group] == i)\n",
    "            y_true_group = y_true[group_idx]\n",
    "            y_pred_group = y_pred[group_idx]\n",
    "            g = group + str(i)\n",
    "            metrics[g] = {}\n",
    "            metrics[g]['statistical_parity'] = np.mean(y_pred_group)\n",
    "            metrics[g]['equalized_odds'] = np.mean(\n",
    "                y_pred_group[y_true_group == 1]) - np.mean(y_pred_group[y_true_group == 0])\n",
    "            metrics[g]['equalized_outcome'] = np.mean(\n",
    "                y_pred_group[y_true_group == 1])\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_balanced_accuracy_groups(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Calculate balanced accuracy for all groups\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for group in groups:\n",
    "        for i in [0, 1]:\n",
    "            group_idx = np.where(groups[group] == i)\n",
    "            y_true_group = y_true[group_idx]\n",
    "            y_pred_group = y_pred[group_idx]\n",
    "            g = group + str(i)\n",
    "            metrics[g] = {}\n",
    "            metrics[g]['balanced_accuracy'] = calculate_metrics(\n",
    "                y_true_group, y_pred_group)['accuracy']\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree classifier object\n",
    "dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# fit the model to the training data\n",
    "dt.fit(X_train, y_train)\n",
    "# evaluate the model on the test set\n",
    "y_pred = dt.predict(X_test)\n",
    "metric_scores = calculate_metrics(y_test, y_pred)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "fairness_metrics = calculate_fairness_metrics(y_test, y_pred, group_test)\n",
    "for key, value in fairness_metrics.items():\n",
    "    print(key, \":\",  value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate uncertainty by bootstrapping\n",
    "n_bootstraps = 1000\n",
    "bootstrapped_scores = []\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = np.random.randint(low=0, high=len(y_pred), size=len(y_pred))\n",
    "    if len(np.unique(y_test[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = calculate_metrics(y_test[indices], y_pred[indices])['accuracy'] # accuracy\n",
    "    bootstrapped_scores.append(score)\n",
    "\n",
    "print(\"Confidence interval for the accuracy score: [{:0.3f} - {:0.3}]\".format(\n",
    "    np.percentile(bootstrapped_scores, 2.5),\n",
    "    np.percentile(bootstrapped_scores, 97.5)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fair PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression on fair PCA data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics and accuracy scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results amongst all datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan defaulting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('data/loan_default.csv')\n",
    "\n",
    "# Dataset from https://www.kaggle.com/datasets/yasserh/loan-default-dataset?datasetId=1897041&sortBy=voteCount,\n",
    "# Protected attribute is Gender\n",
    "# Object is status, 0 or 1 (default or not)\n",
    "\n",
    "# Drop all rows with nan\n",
    "df_loans = df_original.drop(['Region', 'Security_Type', 'dtir1', 'total_units', 'Secured_by',\n",
    "                             'term', 'open_credit', 'year', 'rate_of_interest', 'Interest_rate_spread',\n",
    "                             'Upfront_charges', 'loan_limit', 'construction_type',\n",
    "                             'co-applicant_credit_type', 'ID'], axis=1)\n",
    "\n",
    "# Drop all rows from column 'Gender' that have 'Sex Not Available'\n",
    "df_loans = df_loans[(df_loans['Gender'] != 'Sex Not Available')\n",
    "                    & (df_loans['Gender'] != 'Joint')]\n",
    "\n",
    "# Replace missing values with mode\n",
    "df_loans['approv_in_adv'].fillna(\n",
    "    df_loans['approv_in_adv'].mode()[0], inplace=True)\n",
    "df_loans['loan_purpose'].fillna(\n",
    "    df_loans['loan_purpose'].mode()[0], inplace=True)\n",
    "df_loans['Neg_ammortization'].fillna(\n",
    "    df_loans['Neg_ammortization'].mode()[0], inplace=True)\n",
    "df_loans['property_value'].fillna(\n",
    "    df_loans['property_value'].mode()[0], inplace=True)\n",
    "df_loans['income'].fillna(df_loans['income'].mode()[0], inplace=True)\n",
    "df_loans['LTV'].fillna(df_loans['LTV'].mode()[0], inplace=True)\n",
    "\n",
    "df_loans.hist(bins=30, figsize=(25, 25))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of categorical columns\n",
    "num_cols = df_loans._get_numeric_data().columns\n",
    "cat_cols = list(set(df_loans.columns) - set(num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dython.nominal.associations(\n",
    "    df_loans, nominal_columns=cat_cols, mark_columns=True, figsize=(12, 12));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distributions for both genders in one plot\n",
    "fig, axes = plt.subplots(19, figsize=(20, 40))\n",
    "for i, feature in enumerate(df_loans.columns):\n",
    "    sns.histplot(data=df_loans, x=feature, hue='Gender',\n",
    "                 ax=axes[i], palette='Set2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode data\n",
    "dummies_df_loans = pd.get_dummies(df_loans, columns=cat_cols, drop_first=True)\n",
    "# remove target variable from features\n",
    "labels = dummies_df_loans.Status\n",
    "features = dummies_df_loans.drop(\"Status\", axis=1)\n",
    "\n",
    "# identify protected groups\n",
    "indices = []\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"Gender\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n",
    "        indices.append(i)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "groups = features.iloc[:, indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column 6 from X_train and X_test and save it as protected variable\n",
    "protected = X_train[:, 6]\n",
    "nonprotected = np.delete(X_train, 6, 1)\n",
    "\n",
    "protected_test = X_test[:, 6]\n",
    "nonprotected_test = np.delete(X_test, 6, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree classifier object\n",
    "dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# fit the model to the training data\n",
    "dt.fit(X_train, y_train)\n",
    "# evaluate the model on the test set\n",
    "y_pred = dt.predict(X_test)\n",
    "metric_scores = calculate_metrics(y_test, y_pred)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "fairness_metrics = calculate_fairness_metrics(y_test, y_pred, group_test)\n",
    "for key, value in fairness_metrics.items():\n",
    "    print(key, \":\",  value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate uncertainty by bootstrapping\n",
    "n_bootstraps = 1000\n",
    "bootstrapped_scores = []\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = np.random.randint(low=0, high=len(y_pred), size=len(y_pred))\n",
    "    if len(np.unique(y_test[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = calculate_metrics(y_test[indices], y_pred[indices])['accuracy'] # accuracy\n",
    "    bootstrapped_scores.append(score)\n",
    "\n",
    "print(\"Confidence interval for the accuracy score: [{:0.3f} - {:0.3}]\".format(\n",
    "    np.percentile(bootstrapped_scores, 2.5),\n",
    "    np.percentile(bootstrapped_scores, 97.5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
