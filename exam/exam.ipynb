{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Fair PCA on different datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import dython\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recidivism = pd.read_csv(f'data/propublica_data_for_fairml.csv')\n",
    "\n",
    "df_recidivism['Caucasian'] = ((df_recidivism['Other'] == 0) & (df_recidivism['African_American'] == 0) & (\n",
    "    df_recidivism['Asian'] == 0) & (df_recidivism['Hispanic'] == 0) & (df_recidivism['Native_American'] == 0)).astype(int)\n",
    "df_recidivism['Between_TwentyFive_And_FourtyFive'] = (\n",
    "    (df_recidivism['Age_Above_FourtyFive'] == 0) & (df_recidivism['Age_Below_TwentyFive'] == 0)).astype(int)\n",
    "df_recidivism['Male'] = (df_recidivism['Female'] == 0).astype(int)\n",
    "\n",
    "# revert one hot encoding\n",
    "races = ['Other', 'African_American', 'Asian',\n",
    "         'Hispanic', 'Native_American', 'Caucasian']\n",
    "df_recidivism['Race'] = df_recidivism[races].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(races, axis=1)\n",
    "\n",
    "genders = ['Female', 'Male']\n",
    "df_recidivism['Gender'] = df_recidivism[genders].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(genders, axis=1)\n",
    "\n",
    "age_group = ['Age_Above_FourtyFive', 'Age_Below_TwentyFive',\n",
    "             'Between_TwentyFive_And_FourtyFive']\n",
    "df_recidivism['Age_Group'] = df_recidivism[age_group].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(age_group, axis=1)\n",
    "\n",
    "df_recidivism = df_recidivism.drop('score_factor', axis=1)\n",
    "df_recidivism = df_recidivism[df_recidivism[\"Race\"].isin(\n",
    "    [\"African_American\", \"Caucasian\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"Race\", \"Gender\", \"Age_Group\"]\n",
    "\n",
    "dython.nominal.associations(\n",
    "    df_recidivism, nominal_columns=cat_cols, mark_columns=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recidivism.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot feature distributions for both genders in one plot\n",
    "fig, axes = plt.subplots(7, figsize=(20, 40))\n",
    "for i, feature in enumerate(df_recidivism.columns):\n",
    "    sns.histplot(data=df_recidivism, x=feature,\n",
    "                 hue='Race', ax=axes[i], palette='Set2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode data\n",
    "dummies_df_recividism = pd.get_dummies(\n",
    "    df_recidivism, columns=cat_cols, drop_first=True)\n",
    "# remove target variable from features\n",
    "labels = dummies_df_recividism.Two_yr_Recidivism\n",
    "features = dummies_df_recividism.drop(\"Two_yr_Recidivism\", axis=1)\n",
    "\n",
    "features = features[[\"Number_of_Priors\", \"Misdemeanor\", \"Age_Group_Age_Below_TwentyFive\",\n",
    "                     \"Age_Group_Between_TwentyFive_And_FourtyFive\", \"Race_Caucasian\", \"Gender_Male\"]]\n",
    "\n",
    "# identify protected groups\n",
    "indices = []\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"Race\" in f) or (\"Gender\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n",
    "        indices.append(i)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "groups = features.iloc[:, indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last columns of our data contains the protected features\n",
    "protected = X_train[:, -2:]\n",
    "nonprotected = X_train[:, :-2]\n",
    "\n",
    "protected_test = X_test[:, -2:]\n",
    "nonprotected_test = X_test[:, :-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        Recall of the positive class in binary classification or weighted\n",
    "        average of the recall of each class for the multiclass task.\n",
    "    precision : float\n",
    "        Precision of the positive class in binary classification or weighted\n",
    "        average of the precision of each class for the multiclass task.\n",
    "    f1_score : float\n",
    "        F1 score of the positive class in binary classification or weighted\n",
    "        average of the F1 score of each class for the multiclass task.\n",
    "    accuracy : float\n",
    "        Accuracy of the positive class in binary classification or weighted\n",
    "        average of the accuracy of each class for the multiclass task.\n",
    "\n",
    "    \"\"\"\n",
    "    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n",
    "    FP = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n",
    "    TN = np.sum(np.logical_and(y_pred == 0, y_true == 0))\n",
    "    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP/(TP+FN) + TN/(TN+FP)) / 2\n",
    "\n",
    "    return recall, precision, f1_score, accuracy\n",
    "\n",
    "\n",
    "# calculate uncertainty by bootstrapping\n",
    "n_bootstraps = 1000\n",
    "bootstrapped_scores = []\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = np.random.randint(low=0, high=len(y_pred), size=len(y_pred))\n",
    "    if len(np.unique(y_test[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = balanced_accuracy(y_test[indices], y_pred[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "\n",
    "print(\"Confidence interval for the accuracy score: [{:0.3f} - {:0.3}]\".format(\n",
    "    np.percentile(bootstrapped_scores, 2.5),\n",
    "    np.percentile(bootstrapped_scores, 97.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate statistical parity, equalized odds and equalized outcome for all groups\n",
    "def calculate_metrics(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Calculate statistical parity, equalized odds and equalized outcome for all groups\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for group in groups:\n",
    "        for i in [0, 1]:\n",
    "            group_idx = np.where(groups[group] == i)\n",
    "            y_true_group = y_true[group_idx]\n",
    "            y_pred_group = y_pred[group_idx]\n",
    "            g = group + str(i)\n",
    "            metrics[g] = {}\n",
    "            metrics[g]['statistical_parity'] = np.mean(y_pred_group)\n",
    "            metrics[g]['equalized_odds'] = np.mean(\n",
    "                y_pred_group[y_true_group == 1]) - np.mean(y_pred_group[y_true_group == 0])\n",
    "            metrics[g]['equalized_outcome'] = np.mean(\n",
    "                y_pred_group[y_true_group == 1])\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_balanced_accuracy_groups(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Calculate balanced accuracy for all groups\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for group in groups:\n",
    "        for i in [0, 1]:\n",
    "            group_idx = np.where(groups[group] == i)\n",
    "            y_true_group = y_true[group_idx]\n",
    "            y_pred_group = y_pred[group_idx]\n",
    "            g = group + str(i)\n",
    "            metrics[g] = {}\n",
    "            metrics[g]['balanced_accuracy'] = balanced_accuracy(\n",
    "                y_true_group, y_pred_group)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race_Caucasian0 : {'statistical_parity': 0.60625, 'equalized_odds': 0.3315855267726926, 'equalized_outcome': 0.7706611570247934}\n",
      "Race_Caucasian1 : {'statistical_parity': 0.3301282051282051, 'equalized_odds': 0.21348021073714463, 'equalized_outcome': 0.46218487394957986}\n",
      "Gender_Male0 : {'statistical_parity': 0.21604938271604937, 'equalized_odds': 0.28079116612061644, 'equalized_outcome': 0.3902439024390244}\n",
      "Gender_Male1 : {'statistical_parity': 0.5698412698412698, 'equalized_odds': 0.29807116752833135, 'equalized_outcome': 0.7262103505843072}\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "metrics = calculate_metrics(y_test, y_pred, group_test)\n",
    "for key, value in metrics.items():\n",
    "    print(key, \":\",  value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create decision tree classifier object\n",
    "dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# fit the model to the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = balanced_accuracy(y_test, y_pred)\n",
    "print(\"Balanced accuracy on test set:\", accuracy)\n",
    "\n",
    "# calculate precision by hand\n",
    "TP = np.sum(np.logical_and(y_pred == 1, y_test == 1))\n",
    "FP = np.sum(np.logical_and(y_pred == 1, y_test == 0))\n",
    "precision = TP / (TP + FP)\n",
    "print(\"Precision on test set:\", precision)\n",
    "\n",
    "# calculate recall by hand\n",
    "TP = np.sum(np.logical_and(y_pred == 1, y_test == 1))\n",
    "FN = np.sum(np.logical_and(y_pred == 0, y_test == 1))\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Recall on test set:\", recall)\n",
    "\n",
    "# calculate F1 score by hand\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 score on test set:\", F1)\n",
    "\n",
    "# calculate metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "metrics = calculate_metrics(y_test, y_pred, group_test)\n",
    "for key, value in metrics.items():\n",
    "    print(key, \":\",  value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fair PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression on fair PCA data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics and accuracy scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results amongst all datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan defaulting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('data/loan_default.csv')\n",
    "\n",
    "# Dataset from https://www.kaggle.com/datasets/yasserh/loan-default-dataset?datasetId=1897041&sortBy=voteCount,\n",
    "# Protected attribute is Gender\n",
    "# Object is status, 0 or 1 (default or not)\n",
    "\n",
    "# Drop all rows with nan\n",
    "df_loans = df_original.drop(['Region', 'Security_Type', 'dtir1', 'total_units', 'Secured_by',\n",
    "                             'term', 'open_credit', 'year', 'rate_of_interest', 'Interest_rate_spread',\n",
    "                             'Upfront_charges', 'loan_limit', 'construction_type',\n",
    "                             'co-applicant_credit_type', 'ID'], axis=1)\n",
    "\n",
    "# Drop all rows from column 'Gender' that have 'Sex Not Available'\n",
    "df_loans = df_loans[(df_loans['Gender'] != 'Sex Not Available')\n",
    "                    & (df_loans['Gender'] != 'Joint')]\n",
    "\n",
    "# Replace missing values with mode\n",
    "df_loans['approv_in_adv'].fillna(\n",
    "    df_loans['approv_in_adv'].mode()[0], inplace=True)\n",
    "df_loans['loan_purpose'].fillna(\n",
    "    df_loans['loan_purpose'].mode()[0], inplace=True)\n",
    "df_loans['Neg_ammortization'].fillna(\n",
    "    df_loans['Neg_ammortization'].mode()[0], inplace=True)\n",
    "df_loans['property_value'].fillna(\n",
    "    df_loans['property_value'].mode()[0], inplace=True)\n",
    "df_loans['income'].fillna(df_loans['income'].mode()[0], inplace=True)\n",
    "df_loans['LTV'].fillna(df_loans['LTV'].mode()[0], inplace=True)\n",
    "\n",
    "df_loans.hist(bins=30, figsize=(25, 25))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of categorical columns\n",
    "num_cols = df_loans._get_numeric_data().columns\n",
    "cat_cols = list(set(df_loans.columns) - set(num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dython.nominal.associations(\n",
    "    df_loans, nominal_columns=cat_cols, mark_columns=True, figsize=(12, 12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distributions for both genders in one plot\n",
    "fig, axes = plt.subplots(19, figsize=(20, 40))\n",
    "for i, feature in enumerate(df_loans.columns):\n",
    "    sns.histplot(data=df_loans, x=feature, hue='Gender',\n",
    "                 ax=axes[i], palette='Set2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode data\n",
    "dummies_df_loans = pd.get_dummies(df_loans, columns=cat_cols, drop_first=True)\n",
    "# remove target variable from features\n",
    "labels = dummies_df_loans.Status\n",
    "features = dummies_df_loans.drop(\"Status\", axis=1)\n",
    "\n",
    "# identify protected groups\n",
    "indices = []\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"Gender\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n",
    "        indices.append(i)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "groups = features.iloc[:, indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column 6 from X_train and X_test and save it as protected variable\n",
    "protected = X_train[:, 6]\n",
    "nonprotected = np.delete(X_train, 6, 1)\n",
    "\n",
    "protected_test = X_test[:, 6]\n",
    "nonprotected_test = np.delete(X_test, 6, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalties = [0.001, 0.01, 0.1, 1, 5]\n",
    "best_penalty = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(\"L2-penalty:\", penalty)\n",
    "    accuracy_avg = cross_validate(nonprotected, y_train, penalty)\n",
    "    print(\"Accuracy:\", accuracy_avg)\n",
    "    if accuracy_avg > best_accuracy:\n",
    "        best_accuracy = accuracy_avg\n",
    "        best_penalty = penalty\n",
    "\n",
    "print(\"Best L2-penalty:\", best_penalty)\n",
    "print(\"Best accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the overall performance of the best model on the Test Set (use appropriate metrics) + report uncertainty\n",
    "# function for balanced accuracy\n",
    "beta = np.random.rand(nonprotected.shape[1])\n",
    "result = opt.fmin_tnc(func=compute_cost, x0=beta, approx_grad=True, maxfun=1000,\n",
    "                      args=(nonprotected, y_train, best_penalty), ftol=1e-4, xtol=1e-4)\n",
    "beta = result[0]\n",
    "\n",
    "# evaluate the model on the test set using balanced accuracy\n",
    "y_pred = sigmoid(nonprotected_test.dot(beta))\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "accuracy = balanced_accuracy(y_test, y_pred)\n",
    "print(\"Balanced accuracy on test set:\", accuracy)\n",
    "\n",
    "# calculate precision by hand\n",
    "TP = np.sum(np.logical_and(y_pred == 1, y_test == 1))\n",
    "FP = np.sum(np.logical_and(y_pred == 1, y_test == 0))\n",
    "precision = TP / (TP + FP)\n",
    "print(\"Precision on test set:\", precision)\n",
    "\n",
    "# calculate recall by hand\n",
    "TP = np.sum(np.logical_and(y_pred == 1, y_test == 1))\n",
    "FN = np.sum(np.logical_and(y_pred == 0, y_test == 1))\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Recall on test set:\", recall)\n",
    "\n",
    "# calculate F1 score by hand\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 score on test set:\", F1)\n",
    "\n",
    "# calculate uncertainty by bootstrapping\n",
    "n_bootstraps = 1000\n",
    "bootstrapped_scores = []\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = np.random.randint(low=0, high=len(y_pred), size=len(y_pred))\n",
    "    if len(np.unique(y_test[indices])) < 2:\n",
    "        # We need at least one positive and one negative sample for ROC AUC\n",
    "        # to be defined: reject the sample\n",
    "        continue\n",
    "\n",
    "    score = balanced_accuracy(y_test[indices], y_pred[indices])\n",
    "    bootstrapped_scores.append(score)\n",
    "\n",
    "print(\"Confidence interval for the accuracy score: [{:0.3f} - {:0.3}]\".format(\n",
    "    np.percentile(bootstrapped_scores, 2.5),\n",
    "    np.percentile(bootstrapped_scores, 97.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "metrics = calculate_metrics(y_test_, y_pred, group_test)\n",
    "for key, value in metrics.items():\n",
    "    print(key, \":\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"Gender_Male\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
