{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Fair PCA on different datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import dython\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.tree as tree\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recidivism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recidivism = pd.read_csv(f'data/propublica_data_for_fairml.csv')\n",
    "\n",
    "df_recidivism['Caucasian'] = ((df_recidivism['Other'] == 0) & (df_recidivism['African_American'] == 0) & (\n",
    "    df_recidivism['Asian'] == 0) & (df_recidivism['Hispanic'] == 0) & (df_recidivism['Native_American'] == 0)).astype(int)\n",
    "df_recidivism['Between_TwentyFive_And_FourtyFive'] = (\n",
    "    (df_recidivism['Age_Above_FourtyFive'] == 0) & (df_recidivism['Age_Below_TwentyFive'] == 0)).astype(int)\n",
    "df_recidivism['Male'] = (df_recidivism['Female'] == 0).astype(int)\n",
    "\n",
    "# revert one hot encoding\n",
    "races = ['Other', 'African_American', 'Asian', 'Hispanic', 'Native_American', 'Caucasian']\n",
    "df_recidivism['Race'] = df_recidivism[races].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(races, axis=1)\n",
    "\n",
    "genders = ['Female', 'Male']\n",
    "df_recidivism['Gender'] = df_recidivism[genders].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(genders, axis=1)\n",
    "\n",
    "age_group = ['Age_Above_FourtyFive', 'Age_Below_TwentyFive', 'Between_TwentyFive_And_FourtyFive']\n",
    "df_recidivism['Age_Group'] = df_recidivism[age_group].idxmax(axis=1)\n",
    "df_recidivism = df_recidivism.drop(age_group, axis=1)\n",
    "\n",
    "df_recidivism = df_recidivism.drop('score_factor', axis=1)\n",
    "df_recidivism = df_recidivism[df_recidivism[\"Race\"].isin([\"African_American\", \"Caucasian\"])]\n",
    "df_recidivism = df_recidivism.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"Race\", \"Gender\", \"Age_Group\"]\n",
    "dython.nominal.associations(df_recidivism, nominal_columns=cat_cols, mark_columns=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distributions for both genders in one plot\n",
    "fig, axes = plt.subplots(len(df_recidivism.columns), figsize=(20, 40))\n",
    "for i, feature in enumerate(df_recidivism.columns):\n",
    "    sns.histplot(data=df_recidivism, x=feature, hue='Race', ax=axes[i], palette='Set2')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode data\n",
    "dummies_df_recividism = pd.get_dummies(df_recidivism, columns=cat_cols, drop_first=True)\n",
    "# remove target variable from features\n",
    "labels = dummies_df_recividism.Two_yr_Recidivism\n",
    "features = dummies_df_recividism.drop(\"Two_yr_Recidivism\", axis=1)\n",
    "\n",
    "features = features[[\"Number_of_Priors\", \"Misdemeanor\", \"Age_Group_Age_Below_TwentyFive\",\n",
    "                     \"Age_Group_Between_TwentyFive_And_FourtyFive\", \"Race_Caucasian\", \"Gender_Male\"]]\n",
    "\n",
    "# identify protected groups\n",
    "indices = []\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"Race\" in f) or (\"Gender\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n",
    "        indices.append(i)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "groups = features.iloc[:, indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last columns of our data contains the protected features\n",
    "protected = X_train[:, -2:]\n",
    "nonprotected = X_train[:, :-2]\n",
    "\n",
    "protected_test = X_test[:, -2:]\n",
    "nonprotected_test = X_test[:, :-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        Recall of the positive class in binary classification or weighted\n",
    "        average of the recall of each class for the multiclass task.\n",
    "    precision : float\n",
    "        Precision of the positive class in binary classification or weighted\n",
    "        average of the precision of each class for the multiclass task.\n",
    "    f1_score : float\n",
    "        F1 score of the positive class in binary classification or weighted\n",
    "        average of the F1 score of each class for the multiclass task.\n",
    "    accuracy : float\n",
    "        Accuracy of the positive class in binary classification or weighted\n",
    "        average of the accuracy of each class for the multiclass task.\n",
    "    \"\"\"\n",
    "\n",
    "    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n",
    "    FP = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n",
    "    TN = np.sum(np.logical_and(y_pred == 0, y_true == 0))\n",
    "    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP/(TP+FN) + TN/(TN+FP)) / 2\n",
    "\n",
    "    metrics_dict = {'recall': recall,\n",
    "                    'precision': precision,\n",
    "                    'f1_score': f1_score,\n",
    "                    'accuracy': accuracy}\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate statistical parity, equalized odds and equalized outcome for all groups\n",
    "def calculate_fairness_metrics(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Calculate statistical parity, equalized odds and equalized outcome for all groups\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for group in groups:\n",
    "        group1_idx = np.where(groups[group] == 0)\n",
    "        group2_idx = np.where(groups[group] == 1)\n",
    "        y_true_group1 = y_true[group1_idx]\n",
    "        y_true_group2 = y_true[group2_idx]\n",
    "        y_pred_group1 = y_pred[group1_idx]\n",
    "        y_pred_group2 = y_pred[group2_idx]\n",
    "        tpr_group1 = np.mean(y_pred_group1[y_true_group1 == 1] == 1)\n",
    "        tpr_group2 = np.mean(y_pred_group2[y_true_group2 == 1] == 1)\n",
    "        fpr_group1 = np.mean(y_pred_group1[y_true_group1 == 0] == 1)\n",
    "        fpr_group2 = np.mean(y_pred_group2[y_true_group2 == 0] == 1)\n",
    "        fnr_group1 = np.mean(y_pred_group1[y_true_group1 == 1] == 0)\n",
    "        fnr_group2 = np.mean(y_pred_group2[y_true_group2 == 1] == 0)\n",
    "        tnr_group1 = np.mean(y_pred_group1[y_true_group1 == 0] == 0)\n",
    "        tnr_group2 = np.mean(y_pred_group2[y_true_group2 == 0] == 0)\n",
    "\n",
    "        equalized_odds_tpr = np.abs(tpr_group1 - tpr_group2)\n",
    "        equalized_odds_fpr = np.abs(fpr_group1 - fpr_group2)\n",
    "\n",
    "        metrics[group] = {}\n",
    "        metrics[group]['equalized_odds_tpr'] = equalized_odds_tpr\n",
    "        metrics[group]['equalized_odds_fpr'] = equalized_odds_fpr\n",
    "        metrics[group]['equalized_odds'] = min(equalized_odds_tpr, equalized_odds_fpr)\n",
    "        metrics[group][0] = {}\n",
    "        metrics[group][1] = {}\n",
    "        metrics[group][0]['tpr'] = tpr_group1\n",
    "        metrics[group][0]['fpr'] = fpr_group1\n",
    "        metrics[group][1]['tpr'] = tpr_group2\n",
    "        metrics[group][1]['fpr'] = fpr_group2\n",
    "        metrics[group][0]['fnr'] = fnr_group1\n",
    "        metrics[group][0]['tnr'] = tnr_group1\n",
    "        metrics[group][1]['fnr'] = fnr_group2\n",
    "        metrics[group][1]['tnr'] = tnr_group2\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_balanced_accuracy_groups(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Calculate balanced accuracy for all groups\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for group in groups:\n",
    "        for i in [0, 1]:\n",
    "            group_idx = np.where(groups[group] == i)\n",
    "            y_true_group = y_true[group_idx]\n",
    "            y_pred_group = y_pred[group_idx]\n",
    "            g = group + str(i)\n",
    "            metrics[g] = {}\n",
    "            metrics[g]['balanced_accuracy'] = calculate_metrics(\n",
    "                y_true_group, y_pred_group)['accuracy']\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 1))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline = Pipeline([('scaler', scaler), ('xgb', xgb)])\n",
    "xgb_pipeline.fit(nonprotected, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(nonprotected_test)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = xgb_pipeline['scaler'].transform(nonprotected)\n",
    "X_test_ = xgb_pipeline['scaler'].transform(nonprotected_test)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(xgb_pipeline['xgb'], X_train_, feature_names=features.columns[:-2])\n",
    "shap_values = explainer(X_test_)\n",
    "shap.plots.beeswarm(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "fairness_metrics = calculate_fairness_metrics(y_test, y_pred_xgb, group_test)\n",
    "for key, value in fairness_metrics.items():\n",
    "    print(key, \":\",  value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairlearn.metrics as flm\n",
    "equalized_odds_race = flm.equalized_odds_ratio(y_test, y_pred_xgb, sensitive_features=group_test[\"Race_Caucasian\"])\n",
    "equalized_odds_gender = flm.equalized_odds_ratio(y_test, y_pred_xgb, sensitive_features=group_test[\"Gender_Male\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(equalized_odds_race)\n",
    "print(equalized_odds_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate uncertainty by bootstrapping\n",
    "def calc_certainty(y_test, y_pred):\n",
    "    n_bootstraps = 1000\n",
    "    bootstrapped_scores = []\n",
    "    interval = 0.95\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = np.random.randint(low=0, high=len(y_pred), size=len(y_pred))\n",
    "        if len(np.unique(y_test[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            continue\n",
    "\n",
    "        score = calculate_metrics(y_test[indices], y_pred[indices])['f1_score']\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    print(f\"Confidence interval for the f1_score [{interval}]:\" + \"[{:0.3f} - {:0.3}]\".format(\n",
    "        np.percentile(bootstrapped_scores, ((1-interval)/2)*100),\n",
    "        np.percentile(bootstrapped_scores, (interval + (1-interval)/2)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_certainty(y_test, y_pred_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply unfair PCA\n",
    "def unfair_pca(pca, non_protected_features_scaled):\n",
    "\n",
    "    X_pca = pca.fit_transform(non_protected_features_scaled)\n",
    "\n",
    "    # pca.explained_variance_ratio_\n",
    "    # scree plot bar plot\n",
    "    plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_*100)\n",
    "    for i in range(len(pca.explained_variance_ratio_)):\n",
    "        plt.text(i+0.7, pca.explained_variance_ratio_[i]*100+0.3, str(round(pca.explained_variance_ratio_[i]*100,2))+'%')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.show()\n",
    "\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=(len(features.columns)-4)) # - protected features\n",
    "pca_1 = PCA(n_components=1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# normalize column for number of priors, the others are one hot encoded\n",
    "non_protected_features_scaled = nonprotected.copy()\n",
    "non_protected_features_scaled = scaler.fit_transform(non_protected_features_scaled)\n",
    "\n",
    "X_pca_2 = unfair_pca(pca_2, non_protected_features_scaled)\n",
    "X_pca_1 = unfair_pca(pca_1, non_protected_features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_corr(X_pca, protected, groups, figsize=(24, 3)):\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate((protected, X_pca), axis=1))\n",
    "    corr = df.corr()\n",
    "    \n",
    "    pc_list = [\"PC\" + str(i) for i in range(1, X_pca.shape[1]+1)]\n",
    "    corr.columns = groups + pc_list\n",
    "\n",
    "    rows_to_plot = [x for x in corr.columns if \"PC\" in x]\n",
    "    indices_to_plot = range(len(groups))\n",
    "    corr = corr.loc[indices_to_plot, rows_to_plot]\n",
    "    indices = groups\n",
    "    corr.index = indices\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr,\n",
    "                cmap='RdBu_r',\n",
    "                annot=True,\n",
    "                linewidth=0.5,\n",
    "                vmin=-0.2,\n",
    "                vmax=0.2,\n",
    "                fmt='.2f');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_corr(X_pca_2, protected, list(groups.columns))\n",
    "plot_pca_corr(X_pca_1, protected, list(groups.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(X_pca_test, pca, non_protected_features_scaled_test, protected):\n",
    "    # project test data with pca\n",
    "    X_pca_test = pca.transform(non_protected_features_scaled_test)\n",
    "\n",
    "    # project it back into the original space\n",
    "    X_reconstructed_test = pca.inverse_transform(X_pca_test)\n",
    "\n",
    "    # calculate reconstruction error for each sample as mean absolute error\n",
    "    reconstruction_error_test = []\n",
    "    for i in range(len(non_protected_features_scaled_test)):\n",
    "        reconstruction_error_test.append(np.mean(np.abs(non_protected_features_scaled_test[i] - X_reconstructed_test[i])))\n",
    "\n",
    "    # get reconstruction error for protected features\n",
    "    female_error = []\n",
    "    male_error = []\n",
    "    whites_error = []\n",
    "    african_american_error = []\n",
    "    for i in range(len(reconstruction_error_test)):\n",
    "\n",
    "        # 1 is white and 0 is african american\n",
    "        if protected[i][0] == 1:\n",
    "            whites_error.append(reconstruction_error_test[i])\n",
    "        else:\n",
    "            african_american_error.append(reconstruction_error_test[i])\n",
    "            \n",
    "        # 1 is male and 0 is female\n",
    "        if protected[i][1] == 1:\n",
    "            male_error.append(reconstruction_error_test[i])\n",
    "        else:\n",
    "            female_error.append(reconstruction_error_test[i])\n",
    "\n",
    "    # calculate mean reconstruction error for each group\n",
    "    print(np.mean(male_error))\n",
    "    print(np.mean(female_error))\n",
    "    print(np.mean(whites_error))\n",
    "    print(np.mean(african_american_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize column for number of priores\n",
    "non_protected_features_scaled_test = nonprotected_test.copy()\n",
    "non_protected_features_scaled_test = scaler.transform(non_protected_features_scaled_test)\n",
    "\n",
    "reconstruction_error(X_pca_2, pca_2, non_protected_features_scaled_test, protected_test)\n",
    "print(\"------------------\")\n",
    "reconstruction_error(X_pca_1, pca_1, non_protected_features_scaled_test, protected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fair PCA\n",
    "import scipy\n",
    "\n",
    "def fair_pca(non_protected_features_scaled, protected, n_components):\n",
    "    X = non_protected_features_scaled.copy()\n",
    "    # create Matrix Z with protected features\n",
    "    Z = protected.copy()\n",
    "    # remove mean from each column\n",
    "    Z = Z - np.mean(Z, axis=0)\n",
    "    # find orthonormal null space spanned by ZTX with scipy.linalg.null_space\n",
    "    R = scipy.linalg.null_space(Z.T @ X)\n",
    "\n",
    "    # Find orthonormal eigenvectors RTXTXR with scipy.linalg.eig\n",
    "    eigvals, eigvecs = scipy.linalg.eig((R.T @ X.T) @ (X @ R))\n",
    "    # sort eigenvectors by eigenvalues\n",
    "    idx = eigvals.argsort()[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:,idx]\n",
    "\n",
    "    # get matrix of first n eigenvectors\n",
    "    L = eigvecs[:,:n_components]\n",
    "\n",
    "    # projection matrix U = RL\n",
    "    U = R @ L\n",
    "\n",
    "    # project data\n",
    "    X_projected = X @ U\n",
    "\n",
    "    return X_projected, Z, U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected_2, Z_2, U_2 = fair_pca(non_protected_features_scaled, protected, 2)\n",
    "X_projected_1, Z_1, U_1 = fair_pca(non_protected_features_scaled, protected, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_corr(X_projected_2, protected, list(groups.columns))\n",
    "plot_pca_corr(X_projected_1, protected, list(groups.columns))\n",
    "plot_pca_corr(X_projected_2, nonprotected, list(features.columns[:-2]))\n",
    "plot_pca_corr(X_projected_1, nonprotected, list(features.columns[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error_fair_pca(X_projected, X_test_projected, U, non_protected_features_scaled_test, protected):\n",
    "\n",
    "    # reproject it back into the original space\n",
    "    X_reconstructed = X_projected @ U.T\n",
    "    X_reconstructed_test = X_test_projected @ U.T\n",
    "\n",
    "    # calculate reconstruction error for each sample as mean absolute error\n",
    "    reconstruction_error = []\n",
    "    for i in range(len(non_protected_features_scaled)):\n",
    "        reconstruction_error.append(np.mean(np.abs(non_protected_features_scaled[i] - X_reconstructed[i])))\n",
    "    reconstruction_error_test = []\n",
    "    for i in range(len(non_protected_features_scaled_test)):\n",
    "        reconstruction_error_test.append(np.mean(np.abs(non_protected_features_scaled_test[i] - X_reconstructed_test[i])))\n",
    "    # get reconstruction error for protected features\n",
    "    female_error = []\n",
    "    male_error = []\n",
    "    whites_error = []\n",
    "    african_american_error = []\n",
    "    for i in range(len(reconstruction_error_test)):\n",
    "\n",
    "        # 1 is white and 0 is african american\n",
    "        if protected[i][0] == 1:\n",
    "            whites_error.append(reconstruction_error_test[i])\n",
    "        else:\n",
    "            african_american_error.append(reconstruction_error_test[i])\n",
    "            \n",
    "        # 1 is male and 0 is female\n",
    "        if protected[i][1] == 1:\n",
    "            male_error.append(reconstruction_error_test[i])\n",
    "        else:\n",
    "            female_error.append(reconstruction_error_test[i])\n",
    "\n",
    "    # calculate mean reconstruction error for each group\n",
    "    print(np.mean(male_error))\n",
    "    print(np.mean(female_error))\n",
    "    print(np.mean(whites_error))\n",
    "    print(np.mean(african_american_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project test data\n",
    "X_test_projected_2 = non_protected_features_scaled_test @ U_2\n",
    "X_test_projected_1 = non_protected_features_scaled_test @ U_1\n",
    "\n",
    "reconstruction_error_fair_pca(X_projected_2, X_test_projected_2, U_2, non_protected_features_scaled_test, protected_test)\n",
    "print(\"------------------\")\n",
    "reconstruction_error_fair_pca(X_projected_1, X_test_projected_1, U_1, non_protected_features_scaled_test, protected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_2 = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler_2 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0,None))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline_2 = Pipeline([('scaler', scaler_2), ('xgb', xgb_2)])\n",
    "xgb_pipeline_2.fit(X_projected_2, y_train)\n",
    "y_pred_xgb_2 = xgb_pipeline_2.predict(X_test_projected_2)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb_2)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = xgb_pipeline_2['scaler'].transform(X_projected_2)\n",
    "X_test_2 = xgb_pipeline_2['scaler'].transform(X_test_projected_2)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_2 = shap.Explainer(xgb_pipeline_2['xgb'], X_train_2, feature_names=[f\"PC{i}\" for i in range(1,len(X_train_2[0])+1)])\n",
    "shap_values_2 = explainer_2(X_test_2)\n",
    "shap.plots.bar(shap_values_2, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "xgb_1 = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler_1 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0,None))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline_1 = Pipeline([('scaler', scaler_1), ('xgb', xgb_1)])\n",
    "xgb_pipeline_1.fit(X_projected_1, y_train)\n",
    "y_pred_xgb_1 = xgb_pipeline_1.predict(X_test_projected_1)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb_1)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = xgb_pipeline_1['scaler'].transform(X_projected_1)\n",
    "X_test_1 = xgb_pipeline_1['scaler'].transform(X_test_projected_1)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_1 = shap.Explainer(xgb_pipeline_1['xgb'], X_train_1, feature_names=[f\"PC{i}\" for i in range(1,len(X_train_1[0])+1)])\n",
    "shap_values_1 = explainer_1(X_test_1)\n",
    "shap.plots.bar(shap_values_1, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_gender = flm.equalized_odds_ratio(y_test, y_pred_xgb_2, sensitive_features=group_test[\"Gender_Male\"])\n",
    "equalized_odds_race = flm.equalized_odds_ratio(y_test, y_pred_xgb_2, sensitive_features=group_test[\"Race_Caucasian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(equalized_odds_gender)\n",
    "print(equalized_odds_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics and accuracy scores\n",
    "# calculate fairness metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "fairness_metrics = calculate_fairness_metrics(y_test, y_pred_xgb_2, group_test)\n",
    "for key, value in fairness_metrics.items():\n",
    "    print(key, \":\",  value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_certainty(y_test, y_pred_xgb_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wage Equality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset from https://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/Stock-Watson-EmpiricalExercises-DataSets.htm\n",
    "# Prottected attribute is 'female'. Target is 'eanwke'\n",
    "\n",
    "# All values are int or float already\n",
    "df_weekly_earn = pd.read_csv('data/employment.csv')\n",
    "\n",
    "# Drop 600 rows of the total 5000 rows, which have nan values\n",
    "df_weekly_earn = df_weekly_earn.dropna()\n",
    "df_weekly_earn = df_weekly_earn[df_weekly_earn[\"race\"] != 3] # drop other race\n",
    "df_weekly_earn['race'] = df_weekly_earn['race'] - 1 # 0 is white, 1 is african american\n",
    "df_weekly_earn = df_weekly_earn.drop(['self', 'private'], axis=1) # drop self employed, private employed cause opposite of \"government\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_no_dummies = df_weekly_earn.copy()\n",
    "# create a new 'state' column by using the column names to map to the desired values\n",
    "df_weekly_no_dummies['state'] = df_weekly_no_dummies.apply(lambda row: 'ne' if row['ne_states'] == 1 else ('so' if row['so_states'] == 1 else ('ce' if row['ce_states'] == 1 else 'we')), axis=1)\n",
    "\n",
    "# drop the original one-hot encoded columns\n",
    "df_weekly_no_dummies = df_weekly_no_dummies.drop(['ne_states', 'so_states', 'ce_states', 'we_states'], axis=1)\n",
    "\n",
    "# create a new 'education' column by using the column names to map to the desired values\n",
    "df_weekly_no_dummies['education'] = df_weekly_no_dummies.apply(lambda row: \"less than high school\" if row['educ_lths'] == 1 else (\"high school\" if row['educ_hs'] == 1 else (\"some college\" if row['educ_somecol'] == 1 else (\"associate's degree\" if row['educ_aa'] == 1 else (\"bachelor's degree\" if row['educ_bac'] == 1 else \"advanced degree\")))), axis=1)\n",
    "\n",
    "# drop the original one-hot encoded columns\n",
    "df_weekly_no_dummies = df_weekly_no_dummies.drop(['educ_lths', 'educ_hs', 'educ_somecol', 'educ_aa', 'educ_bac', 'educ_adv'], axis=1)\n",
    "\n",
    "# get list of categorical columns\n",
    "num_cols = df_weekly_no_dummies._get_numeric_data().columns\n",
    "cat_cols = list(set(df_weekly_no_dummies.columns) - set(num_cols))\n",
    "\n",
    "\n",
    "dython.nominal.associations(\n",
    "    df_weekly_no_dummies, nominal_columns=cat_cols, mark_columns=True, figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(df_weekly_no_dummies.columns), figsize=(20, 70))\n",
    "for i, feature in enumerate(df_weekly_no_dummies.columns):\n",
    "    sns.histplot(data=df_weekly_no_dummies, x=feature, hue='female', ax=axes[i], palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_weekly_earn['earnwke'].median()\n",
    "df_weekly_earn['earnwke'] = (df_weekly_earn['earnwke'] >= median).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target variable from features\n",
    "labels = df_weekly_earn.earnwke\n",
    "features = df_weekly_earn.drop(\"earnwke\", axis=1)\n",
    "\n",
    "# reorder so cloumns with \"Gender\" or \"age\" in them are at the end\n",
    "cols = list(features.columns)\n",
    "protected_cols = []\n",
    "for i, col in enumerate(cols):\n",
    "    if (\"race\" in col) or (\"female\" in col):\n",
    "        protected_cols.append(col)\n",
    "\n",
    "cols = list(set(cols) - set(protected_cols))\n",
    "\n",
    "cols = cols + protected_cols\n",
    "features = features[cols]\n",
    "\n",
    "# identify protected groups\n",
    "indices = []\n",
    "for i, f in enumerate(features.columns):\n",
    "    if (\"race\" in f) or (\"female\" in f):\n",
    "        print(\"Column ID: %s\" % i, \"(%s)\" % f)\n",
    "        indices.append(i)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "groups = features.iloc[:, indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.3, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last columns of our data contains the protected features\n",
    "prot_groups = len(indices)\n",
    "protected = X_train[:, -prot_groups:]\n",
    "nonprotected = X_train[:, :-prot_groups]\n",
    "\n",
    "protected_test = X_test[:, -prot_groups:]\n",
    "nonprotected_test = X_test[:, :-prot_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 1))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline = Pipeline([('scaler', scaler), ('xgb', xgb)])\n",
    "xgb_pipeline.fit(nonprotected, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(nonprotected_test)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = xgb_pipeline['scaler'].transform(nonprotected)\n",
    "X_test_ = xgb_pipeline['scaler'].transform(nonprotected_test)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(xgb_pipeline['xgb'], X_train_, feature_names=features.columns[:-2])\n",
    "shap_values = explainer(X_test_)\n",
    "shap.plots.beeswarm(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_race = flm.demographic_parity_ratio(y_test, y_pred_xgb, sensitive_features=group_test[\"race\"])\n",
    "equalized_odds_gender = flm.demographic_parity_ratio(y_test, y_pred_xgb, sensitive_features=group_test[\"female\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(equalized_odds_race)\n",
    "print(equalized_odds_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "fairness_metrics = calculate_fairness_metrics(y_test, y_pred_xgb, group_test)\n",
    "for key, value in fairness_metrics.items():\n",
    "    print(key, \":\",  value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_certainty(y_test, y_pred_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_14 = PCA(n_components=(len(features.columns)-4)) # - protected features\n",
    "pca_12 = PCA(n_components=(len(features.columns)-6)) # trying different amounts of PCs\n",
    "pca_10 = PCA(n_components=(len(features.columns)-8)) # trying different amounts of PCs\n",
    "pca_8 = PCA(n_components=(len(features.columns)-10)) # trying different amounts of PCs\n",
    "scaler = StandardScaler()\n",
    "# normalize column for number of priors, the others are one hot encoded\n",
    "non_protected_features_scaled = nonprotected.copy()\n",
    "non_protected_features_scaled = scaler.fit_transform(non_protected_features_scaled)\n",
    "\n",
    "X_pca_14 = unfair_pca(pca_14, non_protected_features_scaled)\n",
    "X_pca_12 = unfair_pca(pca_12, non_protected_features_scaled)\n",
    "X_pca_10 = unfair_pca(pca_10, non_protected_features_scaled)\n",
    "X_pca_8 = unfair_pca(pca_8, non_protected_features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_corr(X_pca_14, protected, list(groups))\n",
    "plot_pca_corr(X_pca_12, protected, list(groups))\n",
    "plot_pca_corr(X_pca_10, protected, list(groups))\n",
    "plot_pca_corr(X_pca_8, protected, list(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize column for number of priores\n",
    "non_protected_features_scaled_test = nonprotected_test.copy()\n",
    "non_protected_features_scaled_test = scaler.transform(non_protected_features_scaled_test)\n",
    "\n",
    "reconstruction_error(X_pca_14, pca_14, non_protected_features_scaled_test, protected_test)\n",
    "print(\"--------------------------\")\n",
    "reconstruction_error(X_pca_12, pca_12, non_protected_features_scaled_test, protected_test)\n",
    "print(\"--------------------------\")\n",
    "reconstruction_error(X_pca_10, pca_10, non_protected_features_scaled_test, protected_test)\n",
    "print(\"--------------------------\")\n",
    "reconstruction_error(X_pca_8, pca_8, non_protected_features_scaled_test, protected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected_14, Z_14, U_14 = fair_pca(non_protected_features_scaled, protected, 14)\n",
    "X_projected_12, Z_12, U_12 = fair_pca(non_protected_features_scaled, protected, 12)\n",
    "X_projected_10, Z_10, U_10 = fair_pca(non_protected_features_scaled, protected, 10)\n",
    "X_projected_8, Z_8, U_8 = fair_pca(non_protected_features_scaled, protected, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_corr(X_projected_14, protected, list(groups))\n",
    "plot_pca_corr(X_projected_12, protected, list(groups))\n",
    "plot_pca_corr(X_projected_10, protected, list(groups))\n",
    "plot_pca_corr(X_projected_8, protected, list(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project test data\n",
    "X_test_projected_14 = non_protected_features_scaled_test @ U_14\n",
    "X_test_projected_12 = non_protected_features_scaled_test @ U_12\n",
    "X_test_projected_10 = non_protected_features_scaled_test @ U_10\n",
    "X_test_projected_8 = non_protected_features_scaled_test @ U_8\n",
    "\n",
    "reconstruction_error_fair_pca(X_projected_14, X_test_projected_14, U_14, non_protected_features_scaled_test, protected_test)\n",
    "print(\"----------------------------------\")\n",
    "reconstruction_error_fair_pca(X_projected_12, X_test_projected_12, U_12, non_protected_features_scaled_test, protected_test)\n",
    "print(\"----------------------------------\")\n",
    "reconstruction_error_fair_pca(X_projected_10, X_test_projected_10, U_10, non_protected_features_scaled_test, protected_test)\n",
    "print(\"----------------------------------\")\n",
    "reconstruction_error_fair_pca(X_projected_8, X_test_projected_8, U_8, non_protected_features_scaled_test, protected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_14 = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler_14 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 1))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline_14 = Pipeline([('scaler', scaler_14), ('xgb', xgb_14)])\n",
    "xgb_pipeline_14.fit(X_projected_14, y_train)\n",
    "y_pred_xgb_14 = xgb_pipeline_14.predict(X_test_projected_14)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb_14)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])\n",
    "\n",
    "X_train_14 = xgb_pipeline_14['scaler'].transform(X_projected_14)\n",
    "X_test_14 = xgb_pipeline_14['scaler'].transform(X_test_projected_14)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_14 = shap.Explainer(xgb_pipeline_14['xgb'], X_train_14, feature_names=[f\"PC{i}\" for i in range(1, len(X_train_14[0])+1)])\n",
    "shap_values_14 = explainer_14(X_test_14)\n",
    "shap.plots.beeswarm(shap_values_14, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_12 = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler_12 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 1))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline_12 = Pipeline([('scaler', scaler_12), ('xgb', xgb_12)])\n",
    "xgb_pipeline_12.fit(X_projected_12, y_train)\n",
    "y_pred_xgb_12 = xgb_pipeline_12.predict(X_test_projected_12)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb_12)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])\n",
    "\n",
    "X_train_12 = xgb_pipeline_12['scaler'].transform(X_projected_12)\n",
    "X_test_12 = xgb_pipeline_12['scaler'].transform(X_test_projected_12)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_12 = shap.Explainer(xgb_pipeline_12['xgb'], X_train_12, feature_names=[f\"PC{i}\" for i in range(1, len(X_train_12[0])+1)])\n",
    "shap_values_12 = explainer_12(X_test_12)\n",
    "shap.plots.beeswarm(shap_values_12, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation between the principal components and the attributes\n",
    "plot_pca_corr(X_projected_12, nonprotected, list(features.columns[:-2]), (24,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_10 = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler_10 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 1))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline_10 = Pipeline([('scaler', scaler_10), ('xgb', xgb_10)])\n",
    "xgb_pipeline_10.fit(X_projected_10, y_train)\n",
    "y_pred_xgb_10 = xgb_pipeline_10.predict(X_test_projected_10)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb_10)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])\n",
    "\n",
    "X_train_10 = xgb_pipeline_10['scaler'].transform(X_projected_10)\n",
    "X_test_10 = xgb_pipeline_10['scaler'].transform(X_test_projected_10)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_10 = shap.Explainer(xgb_pipeline_10['xgb'], X_train_10, feature_names=[f\"PC{i}\" for i in range(1, len(X_train_10[0])+1)])\n",
    "shap_values_10 = explainer_10(X_test_10)\n",
    "shap.plots.beeswarm(shap_values_10, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_8 = xgboost.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=0)\n",
    "scaler_8 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 1))], remainder='passthrough') # scale only the first column (Number_of_Priors), leave the remainder untouched\n",
    "xgb_pipeline_8 = Pipeline([('scaler', scaler_8), ('xgb', xgb_8)])\n",
    "xgb_pipeline_8.fit(X_projected_8, y_train)\n",
    "y_pred_xgb_8 = xgb_pipeline_8.predict(X_test_projected_8)\n",
    "metric_scores = calculate_metrics(y_test, y_pred_xgb_8)\n",
    "print(\"Balanced accuracy on test set:\", metric_scores['accuracy'])\n",
    "print(\"Precision on test set:\", metric_scores['precision'])\n",
    "print(\"Recall on test set:\", metric_scores['recall'])\n",
    "print(\"F1 score on test set:\", metric_scores['f1_score'])\n",
    "\n",
    "X_train_8 = xgb_pipeline_8['scaler'].transform(X_projected_8)\n",
    "X_test_8 = xgb_pipeline_8['scaler'].transform(X_test_projected_8)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_8 = shap.Explainer(xgb_pipeline_8['xgb'], X_train_8, feature_names=[f\"PC{i}\" for i in range(1, len(X_train_8[0])+1)])\n",
    "shap_values_8 = explainer_8(X_test_8)\n",
    "shap.plots.beeswarm(shap_values_8, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_certainty(y_test, y_pred_xgb_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_gender = flm.demographic_parity_ratio(y_test, y_pred_xgb_12, sensitive_features=group_test[\"female\"])\n",
    "equalized_odds_race = flm.demographic_parity_ratio(y_test, y_pred_xgb_12, sensitive_features=group_test[\"race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(equalized_odds_race)\n",
    "print(equalized_odds_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fairness metrics and accuracy scores\n",
    "# calculate fairness metrics for all groups\n",
    "y_test_ = np.array([1 if y else 0 for y in y_test])\n",
    "fairness_metrics_14 = calculate_fairness_metrics(y_test, y_pred_xgb_14, group_test)\n",
    "for key, value in fairness_metrics_14.items():\n",
    "    print(key, \":\",  value)\n",
    "print(\"----------------------------------\")\n",
    "fairness_metrics_12 = calculate_fairness_metrics(y_test, y_pred_xgb_12, group_test)\n",
    "for key, value in fairness_metrics_12.items():\n",
    "    print(key, \":\",  value)\n",
    "print(\"----------------------------------\")\n",
    "fairness_metrics_10 = calculate_fairness_metrics(y_test, y_pred_xgb_10, group_test)\n",
    "for key, value in fairness_metrics_10.items():\n",
    "    print(key, \":\",  value)\n",
    "print(\"----------------------------------\")\n",
    "fairness_metrics_8 = calculate_fairness_metrics(y_test, y_pred_xgb_8, group_test)\n",
    "for key, value in fairness_metrics_8.items():\n",
    "    print(key, \":\",  value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
